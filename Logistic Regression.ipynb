{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression #\n",
    "\n",
    "It is similar to Linear Regression, but it can predict if something is True or False. It's use for <strong>Classification</strong>\n",
    "\n",
    "### it predicts a probability of an event to happen###\n",
    "\n",
    "https://www.youtube.com/watch?v=gNhogKJ_q7U\n",
    "\n",
    "<img src=\"log_assets/linear_vs_logistic_regression.jpg\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture above shows something very difficult for the Linear Regression.\n",
    "\n",
    "Our data are not well distributed in the X Y axis, but up to a certain value of X, Y is 0 (False) and over a certain value of X, Y becomes = 1 (True)\n",
    "\n",
    "In the picture on the left we try to fit a line across the data. we have 2 problems here:\n",
    "\n",
    "1. the predicted value of Y can exceed 0 and 1...<br/>\n",
    "2. the error between each point and the line is really big, and there is no way we can minimize it with a better <strong>straight (Linear) line</strong>.\n",
    "\n",
    "### We need a line with a different shape. a shape that can better represent our data ###\n",
    "\n",
    "in the right picture you see the logistic regression function. It's name is <strong> SIGMOID</strong>\n",
    "\n",
    "### $$s = {1 \\over 1 + e^{-X}}$$ ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can have simple model, with just one variable (like for Linear regression) or multiple variable.\n",
    "\n",
    "for instance we can predict Mice obesity using weight ( <strong>one variable</strong>) <br/>\n",
    "or we can predict obesity using multiple variables: Weight + Genotype + Age + etc.. ( <strong>multiple variables</strong>)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a very popular tool in Machine Learning and Deep Learning\n",
    "\n",
    "like Linear Regression, we need to fin the best fitting line. In Linear Regression we used 2 techniques: <br/>\n",
    "\n",
    "1. Loss Function - to calculate the score of the line\n",
    "2. Gradient Descent - to find the best line\n",
    "\n",
    "\n",
    "In Logistic Regression this process is the same, but with some big differences.\n",
    "\n",
    "Let's start with the Loss Function. In few words we can say that we cannot use the same function we use in Linear Regression (MSE), also we cannot use the Gradient Descent on the sigmoid function... so how to we find the best fitting line?\n",
    "\n",
    "The idea is pretty simple, what if we change the Y axis ?... what if we could draw a straight line instead of the sigmoid.. working on a straight line is easier than working on a s-shaped line.\n",
    "\n",
    "so we change the unit of measure of the Y axis.. so that we can stretch our S-shaped line. Of course the values we get on our Y axis change their meaning.\n",
    "\n",
    "\n",
    "<img src=\"log_assets/log_odds1.png\"/>\n",
    "\n",
    "the Y axis represent the log odds.... this could be very complicated and you can learn more here: https://www.youtube.com/watch?v=vN5cNN2-HWE&t=772s\n",
    "\n",
    "\n",
    "But if do not want to get into too complicated math, you can simply think that by changing the meaning of the Y axis, we can simplify our process.\n",
    "\n",
    "now we can calculate the <strong>Loss Function</strong>.\n",
    "\n",
    "in Lienar Regression, we were just summing the distance of the points from the line.... here this approach does not work very well, because the distance from the points to the line is infinite.\n",
    "\n",
    "<img src=\"log_assets/log_odds2.png\"/>\n",
    "\n",
    "Instead we project the points on the line, this will give us the log(odds) of each point... just one little step ahead.\n",
    "\n",
    "<img src=\"log_assets/log_odds3.png\"/>\n",
    "\n",
    "\n",
    "We now convert the log odds of each point to the probability... so we are going back to the original line.\n",
    "we use the formula:<br/><br/>\n",
    "\n",
    "### $$p = {e^{log(odds)} \\over 1 + e^{log(odds)}}$$ ###\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<img src=\"log_assets/log_odds4.png\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood ##\n",
    "\n",
    "\n",
    "to calculate the likelihood (Cost Function) we multiply all the values on the Y axis of the ponts (left picture).\n",
    "\n",
    "you can see we are getting the probabilities... each point will have a number between 0 and 1.\n",
    "\n",
    "probability work in a particular way, so that the probability of something NOT happening is the equivalent of 1 - probability of happening...\n",
    "\n",
    "so that point that we know are probability = 1 are simply multiplied together (all blue points)\n",
    "points that we know are probability  = 0 are multiplied using the equivalent for 1 - p (all red points)\n",
    "\n",
    "at the end we get something like this:\n",
    "\n",
    "<strong>likelihood = blue1 * blue2 * blue4 * blue5 * (1 - red1) * (1 - red2) * (1 - red3) * (1 - red4)</strong>\n",
    "\n",
    "Now we make another step forward to simplify things... instead of just calculating this, we apply the log... why?\n",
    "because given the properies of the log, it will make things simpler. This is a sort of optimization.\n",
    "\n",
    "<strong> log(likelihood) = log(blue1 * blue2 * blue4 * blue5 * (1 - red1) * (1 - red2) * (1 - red3) * (1 - red4))</strong>\n",
    "\n",
    "and by a property of the log (here is the optimization), the log of a product becomes the sum of the logs.. like this:\n",
    "\n",
    "<strong> log(likelihood) = log(blue1) + log(blue2) + log(blue4) + log(blue5) + log(1 - red1) + log(1 - red2) + log(1 - red3) + log(1 - red4)</strong>\n",
    "\n",
    "the result is a number... i.e. -2.9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Maximum Likelihood ##\n",
    "\n",
    "We then draw another line an we repeat the process:\n",
    "\n",
    "<img src=\"log_assets/log_odds2.png\"/>\n",
    "\n",
    "and we get another number, let's say -3.6\n",
    "\n",
    "we compare the 2 numbers and we take the biggest one.. this the the line (sigmoid function) that best represent our dataset.\n",
    "\n",
    "\n",
    "Again, drawing a new line by trials and erros is not very efficient. as in Linear Regression we need to use a function to guide us in the right direction: the Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent ###\n",
    "\n",
    "To minimize our cost, we use Gradient Descent. There are other more sophisticated optimization algorithms out there such as conjugate gradient like BFGS \n",
    "\n",
    "\n",
    "Like in Linear regression we take the partial derivatives of the cost function, we multiply by a learning rate and we substract from the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Larning - Python3",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
